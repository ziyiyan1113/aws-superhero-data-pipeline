# Superhero Data Pipeline with AWS Aurora, Lambda, and S3

This project demonstrates a simple serverless data pipeline using AWS services â€” Aurora MySQL, Lambda, and S3 â€” to simulate a real-world database + compute setup.

## ğŸ”§ Tech Stack

- **AWS Aurora (MySQL-Compatible)** â€“ Relational database for storing hero data
- **AWS Lambda** â€“ Serverless compute to query and return data
- **Amazon S3** â€“ Stores uploaded CSV data to be loaded into Aurora
- **EC2** â€“ Used for setup and testing via terminal
- **IAM** â€“ Roles & policies to allow S3 â†’ Aurora integration
- **Redis (optional)** â€“ For caching and performance improvement
- **Python** â€“ Used in Lambda with `pymysql` and `redis` packages

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ lambda_function.py # Main Lambda function handler
â”œâ”€â”€ lambda_layer.zip # Layer containing pymysql and redis
â”œâ”€â”€ week5_data.csv # Main CSV data uploaded to S3
â”œâ”€â”€ test.csv # Sample CSV for testing S3 â†’ Aurora connection
â”œâ”€â”€ README.md # This documentation

---

## âœ… Whatâ€™s Implemented

### âœ”ï¸ Data Setup

- [x] Created Aurora MySQL DB Cluster (`ziyi-cluster`)
- [x] Created `heroes` database and `hero_data` table
- [x] Uploaded sample `test.csv` and real `week5_data.csv` to S3
- [x] Verified S3 path and bucket (`ziyi-assignment-bucket`)
- [x] Assigned appropriate IAM role to Aurora (via parameter group)
- [x] Loaded CSV data into Aurora from S3 using:
  ```sql
  LOAD DATA FROM S3 's3://ziyi-assignment-bucket/week5_data.csv'
  INTO TABLE hero_data
  FIELDS TERMINATED BY ','
  LINES TERMINATED BY '\n'
  IGNORE 1 ROWS;
  
âœ”ï¸ Lambda Setup

 - [x] Created Lambda function with Aurora connection
 - [x] Built lambda_layer.zip with pymysql and redis
 - [x] Attached layer to Lambda
 - [x] Tested Lambda with sample event and confirmed successful response

ğŸ§ª Sample Output

{
  "statusCode": 200,
  "body": {
    "results": [
      { "id": 1, "name": "irondoctor", "hero": "no", "power": "eat", "xp": 1000000, "color": "yellow" },
      { "id": 2, "name": "windhero", "hero": "no", "power": "grow", "xp": 999999, "color": "yellow" },
      { "id": 3, "name": "dogwoman", "hero": "maybe", "power": "steal", "xp": 999998, "color": "pink" }
    ],
    "execution_time": "0.0129s"
  }
}

ğŸš€ Future Improvements

 - [x] Add Redis caching to improve performance for frequent queries
 - [x]  Connect via API Gateway for public HTTP endpoint
 - [x]  Add DynamoDB for NoSQL comparison
 - [x] Visualize data using Streamlit or other front-end framework


ğŸ“Œ Learning Outcomes

 - [x] Set up Aurora DB with S3 integration
 - [x] Handle IAM roles and permissions for secure resource access
 - [x] Work with Lambda layers and external Python packages
 - [x] Execute SQL queries via Lambda using pymysql
 - [x] Build end-to-end AWS-based mini data pipeline


ğŸ“š Resources

AWS Aurora Docs: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/

Lambda Layers: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html

PyMySQL: https://pymysql.readthedocs.io/en/latest/


ğŸ‘¤ Author

Ziyi Yan

